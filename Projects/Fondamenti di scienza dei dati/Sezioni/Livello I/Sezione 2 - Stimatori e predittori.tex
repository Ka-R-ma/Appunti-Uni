\documentclass{subfiles}
\begin{document}\label{sec:2}
L'intero processo di machine learning è basato sulla \emph{teoria dell'apprendimento statistico}.
Secondo tale teoria, i dati che rappresentano un qualunque fenomeno, possono essere visti come appartenenti ad una qualche distribuzione di probabilità ignota;
e per tale motivo si può assumere che essi siano tra loro indipendenti ed equiprobabili,
concludendo pertanto che il valore atteso dei dati di addestramento e quello dei dati di training coincida.
A seconda del tipo di machine learning adoperato (si veda \emph{Sezione \ref{sec:3}}), l'intero processo fa uso di \emph{stimatori \emph{o di} predittori}.
\begin{Definition*}
    uno stimatore, dal punto di vista statistico, è una funzione che, in funzione dei dati, permette di stimare una quantità/funzione interessante dei dati.
\end{Definition*}
Di questi, supposto $\Theta_{n} \text{uno stimatore,} \Theta$ il valore da stimare, si distinguono
\begin{itemize}
    \item \emph{gli stimatori polarizzati:} ossia stimatori tali che
          $$
              \Expected{\Theta_{n}} - \Theta \neq 0
          $$
          Cioè, lo stimatore commette un certo errore nell'approssimare $\Theta$;

    \item \emph{gli stimatori non-polarizzati:} si tratta di stimatori tali che
          $$
              \Expected{\Theta_{n}} - \Theta = 0
          $$
          \begin{Remark*}
              Se si verifica che
              $$
                  \Lim{n}{\infty}{\Theta_{n}} = \Theta
              $$
              si dirà che $\Theta_{n}$ è uno stimatore asintoticamente non polarizzato.
          \end{Remark*}
\end{itemize}
Ulteriori caratteristiche degli stimatori sono la correttezza e la coerenza, nello specifico
$$\begin{aligned}
        \Theta_{n} \text{corretto} & \iff \Expected{\Theta_{n}} = \Theta        \\
        \Theta_{n} \text{coerente} & \iff \Lim{n}{\infty}{\Var{\Theta_{n}}} = 0
    \end{aligned}$$
\begin{Definition*}
    un predittore è un algoritmo della forma
    $$
        y = f(y) + \varepsilon
    $$
    che permette di stimare la funzione $f$ sulla base dei dati di addestramento $x$.
\end{Definition*}
Segue dalla definizione che, affinché $y$ sia buona, l'errore $\varepsilon$ deve essere minimo.

\subsection{MSE e bias-variance decomposition}
\subfile{../Livello II/Sottosezione 2.1 - MSE e BVD.tex}
\clearpage
\end{document}